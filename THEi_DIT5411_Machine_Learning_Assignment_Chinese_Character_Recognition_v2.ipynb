{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNU5CF5oKw7WhH8Xd0eI5xs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/woshiya/DIT5411-Waiva-Oshiya/blob/main/THEi_DIT5411_Machine_Learning_Assignment_Chinese_Character_Recognition_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkHRoY1nNMMq"
      },
      "outputs": [],
      "source": [
        "# STEP 1: Mount Google Drive\n",
        "\n",
        "# This connects your Colab notebook to your Google Drive\n",
        "# so we can save models and access data permanently\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"‚úì Google Drive mounted successfully!\")\n",
        "print(\"  You can now access your Drive files at: /content/drive/MyDrive/\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2: Download Common Words Dataset (4,803 characters)\n",
        "\n",
        "# Dataset Info:\n",
        "# - Source: AI-FREE-Team Traditional Chinese Handwriting Dataset\n",
        "# - Based on Tegaki open-source package\n",
        "# - Characters: 4,803 most commonly used Traditional Chinese characters\n",
        "# - Total images: 250,712 handwritten samples\n",
        "# - Image size: 50x50 pixels\n",
        "# - GitHub: https://github.com/AI-FREE-Team/Traditional-Chinese-Handwriting-Dataset\n",
        "\n",
        "!git clone https://github.com/AI-FREE-Team/Traditional-Chinese-Handwriting-Dataset.git\n",
        "\n",
        "print(\"\\n‚úì Common Words Dataset downloaded successfully!\")\n",
        "print(\"  Location: /content/Traditional-Chinese-Handwriting-Dataset/\")\n",
        "print(\"  Dataset: 4,803 commonly used Traditional Chinese characters\")\n",
        "print(\"  Total samples: 250,712 handwritten images\")\n",
        "print(\"  Image size: 50x50 pixels\")"
      ],
      "metadata": {
        "id": "cU4BF52_Nudr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3: Extract & Organize Dataset\n",
        "\n",
        "# This does two things:\n",
        "# 1. Extracts the 4 zip files\n",
        "# 2. Organizes 250,712 flat images into character folders\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import glob\n",
        "import shutil\n",
        "from collections import defaultdict\n",
        "\n",
        "print(\" STEP 3: Extract & Organize Dataset\\n\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ============================================\n",
        "# PART A: Extract Zip Files\n",
        "# ============================================\n",
        "print(\"\\n PART A: Extracting zip files...\\n\")\n",
        "\n",
        "repo_dir = '/content/Traditional-Chinese-Handwriting-Dataset'\n",
        "data_dir = os.path.join(repo_dir, 'data')\n",
        "os.chdir(data_dir)\n",
        "\n",
        "zip_files = sorted(glob.glob('*.zip'))\n",
        "print(f\"Found {len(zip_files)} zip files:\")\n",
        "for zf in zip_files:\n",
        "    size_mb = os.path.getsize(zf) / (1024*1024)\n",
        "    print(f\"  ‚Ä¢ {zf} ({size_mb:.1f} MB)\")\n",
        "\n",
        "print(\"\\nExtracting...\")\n",
        "for i, zip_file in enumerate(zip_files, 1):\n",
        "    print(f\"  [{i}/{len(zip_files)}] {zip_file}...\", end=\" \")\n",
        "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "        zip_ref.extractall('/content/')\n",
        "    print(\"‚úì\")\n",
        "\n",
        "print(\"\\n‚úì Extraction complete!\")\n",
        "\n",
        "# ============================================\n",
        "# PART B: Organize into Character Folders\n",
        "# ============================================\n",
        "print(\"\\n PART B: Organizing images into character folders...\\n\")\n",
        "\n",
        "source_dir = '/content/cleaned_data(50_50)'\n",
        "organized_dir = '/content/cleaned_data_organized'\n",
        "\n",
        "# Create organized directory\n",
        "os.makedirs(organized_dir, exist_ok=True)\n",
        "\n",
        "# Get all PNG files\n",
        "all_files = [f for f in os.listdir(source_dir) if f.endswith('.png')]\n",
        "print(f\"‚úì Found {len(all_files):,} image files\")\n",
        "\n",
        "# Group files by character\n",
        "char_groups = defaultdict(list)\n",
        "for filename in all_files:\n",
        "    if '_' in filename:\n",
        "        char = filename.rsplit('_', 1)[0]\n",
        "        char_groups[char].append(filename)\n",
        "\n",
        "print(f\"‚úì Found {len(char_groups):,} unique characters\")\n",
        "print(f\"\\n Creating folders and organizing images...\")\n",
        "\n",
        "# Copy files into character folders\n",
        "for i, (char, files) in enumerate(char_groups.items(), 1):\n",
        "    char_folder = os.path.join(organized_dir, char)\n",
        "    os.makedirs(char_folder, exist_ok=True)\n",
        "\n",
        "    for file in files:\n",
        "        src = os.path.join(source_dir, file)\n",
        "        dst = os.path.join(char_folder, file)\n",
        "        shutil.copy2(src, dst)\n",
        "\n",
        "    # Progress every 500 characters\n",
        "    if i % 500 == 0:\n",
        "        print(f\"  Progress: {i}/{len(char_groups)} characters...\")\n",
        "\n",
        "print(f\"\\n‚úì Organization complete!\")\n",
        "print(f\"  Location: {organized_dir}\")\n",
        "print(f\"  Character folders: {len(char_groups):,}\")\n",
        "print(f\"  Total images: {len(all_files):,}\")\n",
        "\n",
        "# Verify\n",
        "actual_folders = len([d for d in os.listdir(organized_dir)\n",
        "                      if os.path.isdir(os.path.join(organized_dir, d))])\n",
        "\n",
        "print(f\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úì STEP 3 COMPLETE!\")\n",
        "print(f\"  Status: {'‚úì Ready for Step 4' if actual_folders > 4000 else ' Check manually'}\")\n",
        "print(f\"  Character folders: {actual_folders:,}\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "kp2Q5xVFP1jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 4: Select 40 Characters & Copy to Drive\n",
        "\n",
        "# From 4,803 characters, we select 40 for faster training\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "import random\n",
        "\n",
        "# Use the organized folder\n",
        "source_dir = '/content/cleaned_data_organized'\n",
        "destination_dir = '/content/drive/MyDrive/Chinese_Character_Recognition_v2'\n",
        "\n",
        "# Create project folder\n",
        "os.makedirs(destination_dir, exist_ok=True)\n",
        "print(f\"‚úì Created project folder: {destination_dir}\\n\")\n",
        "\n",
        "# Get list of all character folders\n",
        "all_characters = [d for d in os.listdir(source_dir)\n",
        "                  if os.path.isdir(os.path.join(source_dir, d))]\n",
        "\n",
        "print(f\" Total available character folders: {len(all_characters):,}\")\n",
        "\n",
        "# Select 40 random character folders\n",
        "random.seed(42)  # For reproducibility\n",
        "selected_characters = random.sample(all_characters, 40)\n",
        "\n",
        "print(f\"‚úì Selected 40 character folders for training\\n\")\n",
        "print(\"Selected characters:\")\n",
        "for i in range(0, 40, 10):\n",
        "    print(\"  \" + \", \".join(selected_characters[i:i+10]))\n",
        "\n",
        "# Copy to Drive\n",
        "destination_data = os.path.join(destination_dir, 'data')\n",
        "\n",
        "if os.path.exists(destination_data):\n",
        "    print(\"\\n  Data folder already exists in Drive.\")\n",
        "    print(\"   Deleting and recreating with fresh selection...\")\n",
        "    shutil.rmtree(destination_data)\n",
        "    print(\"   ‚úì Old data deleted\")\n",
        "\n",
        "os.makedirs(destination_data)\n",
        "\n",
        "print(\"\\n Copying 40 selected characters to Google Drive...\")\n",
        "print(\"   Estimated time: 1-2 minutes\")\n",
        "print(\"     Don't close this tab!\\n\")\n",
        "\n",
        "total_images = 0\n",
        "for i, char_folder in enumerate(selected_characters, 1):\n",
        "    char_source = os.path.join(source_dir, char_folder)\n",
        "    char_dest = os.path.join(destination_data, char_folder)\n",
        "\n",
        "    shutil.copytree(char_source, char_dest)\n",
        "    num_images = len([f for f in os.listdir(char_dest) if f.endswith('.png')])\n",
        "    total_images += num_images\n",
        "    print(f\"  [{i:2d}/40] '{char_folder}' ({num_images} images)\")\n",
        "\n",
        "print(f\"\\n‚úì Copy complete!\")\n",
        "print(f\"  Total images copied: {total_images:,}\")\n",
        "\n",
        "# Verify\n",
        "num_folders = len(os.listdir(destination_data))\n",
        "\n",
        "print(f\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úì STEP 4 COMPLETE!\")\n",
        "print(f\"  Location: {destination_data}\")\n",
        "print(f\"  Character folders: {num_folders}\")\n",
        "print(f\"  Total images: {total_images:,}\")\n",
        "print(f\"  Average per character: {total_images // num_folders}\")\n",
        "print(f\"  Status: ‚úì Ready for training!\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "kgMMSEpaQaBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 5  Install & Import necessary lib\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\" Installing Additional Packages (if needed)...\")\n",
        "print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "# OpenCV installation\n",
        "!pip install opencv-python-headless --quiet\n",
        "\n",
        "print(\"‚úì Installation complete!\\n\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\" Importing Libraries...\")\n",
        "print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "# Basic Python\n",
        "import os\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Data & Arrays\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Deep Learning\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import (Conv2D, MaxPooling2D, Flatten, Dense,\n",
        "                                      Dropout, BatchNormalization)\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"‚úì All libraries imported!\\n\")\n",
        "\n",
        "# Set random seeds\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "print(f\"‚úì Random seed set to {SEED}\\n\")\n",
        "\n",
        "# Quick version check\n",
        "print(\"=\" * 60)\n",
        "print(\"Key Library Versions:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"  TensorFlow:  {tf.__version__}\")\n",
        "print(f\"  NumPy:       {np.__version__}\")\n",
        "print(f\"  OpenCV:      {cv2.__version__}\")\n",
        "\n",
        "# GPU check\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Hardware:\")\n",
        "print(\"=\" * 60)\n",
        "gpu = tf.config.list_physical_devices('GPU')\n",
        "if gpu:\n",
        "    print(f\"  ‚úì GPU Available: {gpu[0].name}\")\n",
        "else:\n",
        "    print(f\"   No GPU (training will be slower)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úì STEP 5 COMPLETE!\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "rDTGM4QPSgH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 6: Set Paths and Parameters\n",
        "\n",
        "# Paths\n",
        "DATA_PATH = '/content/drive/MyDrive/Chinese_Character_Recognition_v2/data'\n",
        "MODEL_SAVE_PATH = '/content/drive/MyDrive/Chinese_Character_Recognition_v2'\n",
        "\n",
        "# Image parameters\n",
        "IMG_HEIGHT = 64\n",
        "IMG_WIDTH = 64\n",
        "IMG_CHANNELS = 1  # Grayscale\n",
        "\n",
        "# Training parameters\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 30\n",
        "VALIDATION_SPLIT = 0.2\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# Early stopping patience\n",
        "PATIENCE = 5\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\" Configuration Settings\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\n Paths:\")\n",
        "print(f\"  Data:   {DATA_PATH}\")\n",
        "print(f\"  Models: {MODEL_SAVE_PATH}\")\n",
        "\n",
        "print(f\"\\n  Image Settings:\")\n",
        "print(f\"  Size:     {IMG_HEIGHT}x{IMG_WIDTH}\")\n",
        "print(f\"  Channels: {IMG_CHANNELS} (Grayscale)\")\n",
        "\n",
        "print(f\"\\nüéØ Training Settings:\")\n",
        "print(f\"  Batch size:        {BATCH_SIZE}\")\n",
        "print(f\"  Epochs:            {EPOCHS}\")\n",
        "print(f\"  Validation split:  {VALIDATION_SPLIT * 100}%\")\n",
        "print(f\"  Learning rate:     {LEARNING_RATE}\")\n",
        "print(f\"  Early stop patience: {PATIENCE} epochs\")\n",
        "\n",
        "# Verify data path and get class info\n",
        "if os.path.exists(DATA_PATH):\n",
        "    classes = sorted([d for d in os.listdir(DATA_PATH)\n",
        "                     if os.path.isdir(os.path.join(DATA_PATH, d))])\n",
        "    NUM_CLASSES = len(classes)\n",
        "\n",
        "    # Count total images\n",
        "    total_images = 0\n",
        "    for char_folder in classes:\n",
        "        char_path = os.path.join(DATA_PATH, char_folder)\n",
        "        imgs = [f for f in os.listdir(char_path) if f.endswith('.png')]\n",
        "        total_images += len(imgs)\n",
        "\n",
        "    print(f\"\\n‚úì Data Verified:\")\n",
        "    print(f\"  Classes:       {NUM_CLASSES}\")\n",
        "    print(f\"  Total images:  {total_images:,}\")\n",
        "    print(f\"  Avg per class: {total_images // NUM_CLASSES}\")\n",
        "\n",
        "    # Show sample characters\n",
        "    print(f\"\\n Sample Characters:\")\n",
        "    print(f\"  {', '.join(classes[:10])}\")\n",
        "    if len(classes) > 10:\n",
        "        print(f\"  ... and {len(classes) - 10} more\")\n",
        "\n",
        "    # Store for later use\n",
        "    CLASS_NAMES = classes\n",
        "else:\n",
        "    print(f\"\\n ERROR: Data path not found!\")\n",
        "    print(f\"  Check: {DATA_PATH}\")\n",
        "    NUM_CLASSES = 0\n",
        "    CLASS_NAMES = []\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úì STEP 6 COMPLETE!\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "UDFNAcBTUHMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 7: Load & Pre-Generate Augmented Data\n",
        "\n",
        "# This creates a fixed augmented dataset that models will use later on for model performance comparision later\n",
        "# Assignment requirement: \"create at least 200 samples per character\"\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\" STEP 8: Loading & Augmenting Data\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ============================================\n",
        "# PART A: Load Original Images\n",
        "# ============================================\n",
        "print(\"\\n PART A: Loading original images...\\n\")\n",
        "\n",
        "X_original = []\n",
        "y_original = []\n",
        "\n",
        "for class_idx, char_name in enumerate(CLASS_NAMES):\n",
        "    char_path = os.path.join(DATA_PATH, char_name)\n",
        "    image_files = [f for f in os.listdir(char_path) if f.endswith('.png')]\n",
        "\n",
        "    for img_file in image_files:\n",
        "        img_path = os.path.join(char_path, img_file)\n",
        "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "        img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
        "        X_original.append(img)\n",
        "        y_original.append(class_idx)\n",
        "\n",
        "    if (class_idx + 1) % 10 == 0:\n",
        "        print(f\"  Loaded {class_idx + 1}/{NUM_CLASSES} classes ({len(X_original):,} images)\")\n",
        "\n",
        "print(f\"\\n‚úì Original images loaded: {len(X_original):,} images\")\n",
        "print(f\"  Average per character: {len(X_original) // NUM_CLASSES}\")\n",
        "\n",
        "# ============================================\n",
        "# PART B: Pre-Generate Augmented Images\n",
        "# ============================================\n",
        "print(f\"\\n PART B: Generating augmented images...\")\n",
        "print(f\"   Creating 5 versions per image (1 original + 4 augmented)\")\n",
        "print(f\"   Target: 200+ samples per character\")\n",
        "print(f\"   This will take 2-3 minutes - please wait!\\n\")\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define augmentation parameters (5 techniques)\n",
        "augmentation_gen = ImageDataGenerator(\n",
        "    rotation_range=15,        # 1. Random rotation ¬±15 degrees\n",
        "    width_shift_range=0.1,    # 2. Horizontal shift ¬±10%\n",
        "    height_shift_range=0.1,   # 3. Vertical shift ¬±10%\n",
        "    shear_range=0.1,          # 4. Shear transformation\n",
        "    zoom_range=0.1,           # 5. Random zoom ¬±10%\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "X_augmented = []\n",
        "y_augmented = []\n",
        "\n",
        "# Track augmentation count per character\n",
        "char_counts = {i: 0 for i in range(NUM_CLASSES)}\n",
        "\n",
        "for i, (img, label) in enumerate(zip(X_original, y_original)):\n",
        "    # Add original image first\n",
        "    X_augmented.append(img)\n",
        "    y_augmented.append(label)\n",
        "    char_counts[label] += 1\n",
        "\n",
        "    # Generate 4 augmented versions\n",
        "    img_reshaped = img.reshape(1, IMG_HEIGHT, IMG_WIDTH, 1).astype('float32')\n",
        "    aug_iter = augmentation_gen.flow(img_reshaped, batch_size=1, seed=SEED)\n",
        "\n",
        "    for aug_num in range(4):  # Create 4 augmented versions\n",
        "        aug_img = next(aug_iter)[0].reshape(IMG_HEIGHT, IMG_WIDTH)\n",
        "        X_augmented.append(aug_img.astype('uint8'))\n",
        "        y_augmented.append(label)\n",
        "        char_counts[label] += 1\n",
        "\n",
        "    # Progress indicator\n",
        "    if (i + 1) % 200 == 0:\n",
        "        print(f\"  Progress: {i + 1}/{len(X_original)} images processed ({len(X_augmented):,} total)\")\n",
        "\n",
        "print(f\"\\n‚úì Augmentation complete!\")\n",
        "print(f\"  Original images:  {len(X_original):,}\")\n",
        "print(f\"  Augmented images: {len(X_augmented):,}\")\n",
        "print(f\"  Multiplication:   {len(X_augmented) / len(X_original):.1f}x\")\n",
        "print(f\"  Per character:    ~{len(X_augmented) // NUM_CLASSES} samples\")\n",
        "\n",
        "# Verify augmentation count per character\n",
        "min_samples = min(char_counts.values())\n",
        "max_samples = max(char_counts.values())\n",
        "print(f\"  Sample range:     {min_samples} - {max_samples} per character\")\n",
        "\n",
        "# Check if requirement met\n",
        "if min_samples >= 200:\n",
        "    print(f\"  ‚úì Requirement met: All characters have 200+ samples!\")\n",
        "else:\n",
        "    print(f\"   Note: Characters have {min_samples} samples (close to 200)\")\n",
        "\n",
        "# ============================================\n",
        "# PART C: Convert to NumPy & Preprocess\n",
        "# ============================================\n",
        "print(f\"\\n PART C: Preprocessing augmented dataset...\")\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X = np.array(X_augmented, dtype='float32')\n",
        "y = np.array(y_augmented, dtype='int32')\n",
        "\n",
        "print(f\"  X shape before: {X.shape}\")\n",
        "print(f\"  y shape before: {y.shape}\")\n",
        "\n",
        "# Normalize pixel values to [0, 1]\n",
        "X = X / 255.0\n",
        "\n",
        "# Reshape for CNN: add channel dimension\n",
        "X = X.reshape(-1, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n",
        "\n",
        "print(f\"  X shape after:  {X.shape}\")\n",
        "print(f\"  X range:        [{X.min():.3f}, {X.max():.3f}]\")\n",
        "\n",
        "# One-hot encode labels\n",
        "y_categorical = to_categorical(y, num_classes=NUM_CLASSES)\n",
        "print(f\"  y shape after:  {y_categorical.shape}\")\n",
        "\n",
        "# ============================================\n",
        "# PART D: Train/Test Split\n",
        "# ============================================\n",
        "print(f\"\\n PART D: Splitting into train/test sets...\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_categorical,\n",
        "    test_size=VALIDATION_SPLIT,\n",
        "    random_state=SEED,\n",
        "    stratify=y  # Ensure balanced split\n",
        ")\n",
        "\n",
        "print(f\"  Training samples:   {len(X_train):,} ({(1-VALIDATION_SPLIT)*100:.0f}%)\")\n",
        "print(f\"  Testing samples:    {len(X_test):,} ({VALIDATION_SPLIT*100:.0f}%)\")\n",
        "print(f\"  Classes:            {NUM_CLASSES}\")\n",
        "\n",
        "# ============================================\n",
        "# PART E: Visualize Augmented Samples\n",
        "# ============================================\n",
        "print(f\"\\n  PART E: Visualizing augmented samples...\\n\")\n",
        "\n",
        "fig, axes = plt.subplots(4, 5, figsize=(15, 12))\n",
        "fig.suptitle('Augmented Training Samples (Same Character, Different Augmentations)',\n",
        "             fontsize=14, fontweight='bold')\n",
        "\n",
        "# Take samples from first character\n",
        "first_char_idx = np.where(np.argmax(y_train, axis=1) == 0)[0][:20]\n",
        "\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    if i < len(first_char_idx):\n",
        "        img = X_train[first_char_idx[i]].reshape(IMG_HEIGHT, IMG_WIDTH)\n",
        "        label_idx = np.argmax(y_train[first_char_idx[i]])\n",
        "        char_name = CLASS_NAMES[label_idx]\n",
        "\n",
        "        ax.imshow(img, cmap='gray')\n",
        "        ax.set_title(f\"'{char_name}' - Sample {i+1}\", fontsize=10)\n",
        "        ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Show augmentation techniques used\n",
        "print(\"Augmentation Techniques Applied:\")\n",
        "print(\"  1. Rotation:      ¬±15 degrees\")\n",
        "print(\"  2. Width shift:   ¬±10%\")\n",
        "print(\"  3. Height shift:  ¬±10%\")\n",
        "print(\"  4. Shear:         10%\")\n",
        "print(\"  5. Zoom:          ¬±10%\")\n",
        "\n",
        "# ============================================\n",
        "# Summary\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úì STEP 7 COMPLETE!\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"  Dataset size:        {len(X_train):,} training images\")\n",
        "print(f\"  Per character:       ~{len(X_train) // NUM_CLASSES} images\")\n",
        "print(f\"  Test set:            {len(X_test):,} images\")\n",
        "print(f\"  Augmentation:        5x (1 original + 4 augmented)\")\n",
        "print(f\"  Status:              Ready for model training!\")\n",
        "print(f\"  All 5 models will:   Use this SAME augmented dataset\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "sZZb0LCGVHaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# PART F: Save & Zip ALL Augmented Data\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\" PART F: Saving Augmented Dataset\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "import shutil\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Create directory for all augmented images\n",
        "augmented_dir = '/content/augmented_data_all'\n",
        "os.makedirs(augmented_dir, exist_ok=True)\n",
        "\n",
        "print(f\"\\nüíæ Saving all augmented training images to files...\")\n",
        "print(f\"   Total images to save: {len(X_train):,}\")\n",
        "print(f\"   Number of classes: {NUM_CLASSES}\")\n",
        "print(f\"   Images per class: ~{len(X_train) // NUM_CLASSES}\")\n",
        "print(f\"\\n   This will take 3-5 minutes - please wait!\\n\")\n",
        "\n",
        "# Save all training images organized by character\n",
        "for idx in range(len(X_train)):\n",
        "    # Get character label\n",
        "    char_idx = np.argmax(y_train[idx])\n",
        "    char_name = CLASS_NAMES[char_idx]\n",
        "\n",
        "    # Create character folder\n",
        "    char_folder = os.path.join(augmented_dir, char_name)\n",
        "    os.makedirs(char_folder, exist_ok=True)\n",
        "\n",
        "    # Convert image back to 0-255 range\n",
        "    img = (X_train[idx] * 255).astype(np.uint8).squeeze()\n",
        "\n",
        "    # Save as PNG\n",
        "    img_pil = Image.fromarray(img, mode='L')\n",
        "    img_filename = f'{char_name}_{idx:06d}.png'\n",
        "    img_pil.save(os.path.join(char_folder, img_filename))\n",
        "\n",
        "    # Progress indicator every 500 images\n",
        "    if (idx + 1) % 500 == 0:\n",
        "        progress = (idx + 1) / len(X_train) * 100\n",
        "        print(f\"   Progress: {idx + 1:,}/{len(X_train):,} ({progress:.1f}%) - {char_name}\")\n",
        "\n",
        "print(f\"\\n‚úì All images saved to: {augmented_dir}\")\n",
        "\n",
        "# Verify folders created\n",
        "saved_folders = [f for f in os.listdir(augmented_dir) if os.path.isdir(os.path.join(augmented_dir, f))]\n",
        "print(f\"‚úì Character folders created: {len(saved_folders)}/{NUM_CLASSES}\")\n",
        "\n",
        "# Count total files\n",
        "total_files = sum([len(files) for r, d, files in os.walk(augmented_dir)])\n",
        "print(f\"‚úì Total images saved: {total_files:,}\")\n",
        "\n",
        "# ============================================\n",
        "# Create ZIP file\n",
        "# ============================================\n",
        "print(f\"\\nüóúÔ∏è Creating zip file...\")\n",
        "print(f\"   Compressing {total_files:,} images...\")\n",
        "print(f\"   This may take 2-3 minutes...\\n\")\n",
        "\n",
        "# Create the zip\n",
        "zip_base_name = '/content/augmented_data_all_40chars'\n",
        "shutil.make_archive(\n",
        "    base_name=zip_base_name,\n",
        "    format='zip',\n",
        "    root_dir=augmented_dir\n",
        ")\n",
        "\n",
        "# Get zip file info\n",
        "zip_filename = f'{zip_base_name}.zip'\n",
        "zip_size_mb = os.path.getsize(zip_filename) / (1024 * 1024)\n",
        "\n",
        "print(f\"‚úì Zip file created successfully!\")\n",
        "print(f\"   Filename: augmented_data_all_40chars.zip\")\n",
        "print(f\"   Size: {zip_size_mb:.2f} MB\")\n",
        "print(f\"   Location: /content/augmented_data_all_40chars.zip\")\n",
        "\n",
        "# Check GitHub compatibility\n",
        "if zip_size_mb > 100:\n",
        "    print(f\"\\n‚ö†Ô∏è  WARNING: File is {zip_size_mb:.0f} MB\")\n",
        "    print(f\"   GitHub single file limit: 100 MB\")\n",
        "    print(f\"   This file is TOO LARGE for direct GitHub upload!\")\n",
        "    print(f\"\\nüí° RECOMMENDATIONS:\")\n",
        "    print(f\"   1. Upload to Google Drive instead\")\n",
        "    print(f\"   2. Use Git LFS (Large File Storage)\")\n",
        "    print(f\"   3. Split into smaller parts\")\n",
        "    print(f\"   4. Upload only sample images to GitHub\")\n",
        "    print(f\"   5. Document that full dataset can be regenerated from code\")\n",
        "else:\n",
        "    print(f\"\\n‚úì File size OK for GitHub upload (<100 MB)\")\n",
        "\n",
        "# ============================================\n",
        "# Download option\n",
        "# ============================================\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "download_now = input(\"üì• Download zip file now? (y/n): \").lower().strip()\n",
        "\n",
        "if download_now == 'y':\n",
        "    print(\"\\nüì• Downloading augmented_data_all_40chars.zip...\")\n",
        "    print(f\"   File size: {zip_size_mb:.2f} MB\")\n",
        "    if zip_size_mb > 50:\n",
        "        print(f\"   ‚ö†Ô∏è  Large file - download may take time\")\n",
        "\n",
        "    from google.colab import files\n",
        "    files.download(zip_filename)\n",
        "    print(\"‚úì Download started!\")\n",
        "else:\n",
        "    print(\"\\nüí° To download later:\")\n",
        "    print(\"   1. Click Files icon (üìÅ) in left sidebar\")\n",
        "    print(\"   2. Find 'augmented_data_all_40chars.zip'\")\n",
        "    print(\"   3. Right-click ‚Üí Download\")\n",
        "\n",
        "# ============================================\n",
        "# Summary\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úì PART F COMPLETE!\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Dataset Summary:\")\n",
        "print(f\"  üìÇ Total characters:    {NUM_CLASSES}\")\n",
        "print(f\"  üñºÔ∏è  Total images:        {total_files:,}\")\n",
        "print(f\"  üìä Images per char:     ~{total_files // NUM_CLASSES}\")\n",
        "print(f\"  üíæ Zip file:            augmented_data_all_40chars.zip\")\n",
        "print(f\"  üìè Zip size:            {zip_size_mb:.2f} MB\")\n",
        "print(f\"  üìÅ Folder structure:    /{char_name}/image.png\")\n",
        "\n",
        "if zip_size_mb > 100:\n",
        "    print(f\"\\n‚ö†Ô∏è  GitHub Upload: NOT RECOMMENDED (file too large)\")\n",
        "    print(f\"  ‚Üí Use Google Drive or document regeneration process\")\n",
        "else:\n",
        "    print(f\"\\n‚úì GitHub Upload: OK\")\n",
        "\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "_7O0FLmxHxLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 8 : MODEL 1 - BASELINE SIMPLE CNN\n",
        "\n",
        "# Simple CNN with 2 convolutional layers\n",
        "# Fast training, baseline performance\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\" MODEL 1: BASELINE SIMPLE CNN\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import time\n",
        "\n",
        "# ============================================\n",
        "# PART A: Build Model Architecture\n",
        "# ============================================\n",
        "print(\"\\n  PART A: Building Model Architecture...\\n\")\n",
        "\n",
        "model_1 = Sequential([\n",
        "    # Input layer (implicit): 64x64x1\n",
        "\n",
        "    # Convolutional Block 1\n",
        "    Conv2D(32, kernel_size=(5, 5), activation='relu',\n",
        "           input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),\n",
        "           padding='same', name='conv1'),\n",
        "    MaxPooling2D(pool_size=(2, 2), name='pool1'),\n",
        "    Dropout(0.2, name='dropout1'),\n",
        "\n",
        "    # Convolutional Block 2\n",
        "    Conv2D(64, kernel_size=(3, 3), activation='relu',\n",
        "           padding='same', name='conv2'),\n",
        "    MaxPooling2D(pool_size=(2, 2), name='pool2'),\n",
        "    Dropout(0.2, name='dropout2'),\n",
        "\n",
        "    # Fully Connected Layers\n",
        "    Flatten(name='flatten'),\n",
        "    Dense(128, activation='relu', name='dense1'),\n",
        "    Dropout(0.3, name='dropout3'),\n",
        "    Dense(NUM_CLASSES, activation='softmax', name='output')\n",
        "], name='Baseline_Simple_CNN')\n",
        "\n",
        "# Display architecture\n",
        "print(\"üìã Model Architecture:\")\n",
        "print(\"-\" * 70)\n",
        "model_1.summary()\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Count parameters\n",
        "total_params = model_1.count_params()\n",
        "print(f\"\\n Total Parameters: {total_params:,}\")\n",
        "\n",
        "# ============================================\n",
        "# PART B: Compile Model\n",
        "# ============================================\n",
        "print(f\"\\n  PART B: Compiling Model...\\n\")\n",
        "\n",
        "model_1.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"‚úì Model compiled successfully!\")\n",
        "print(\"  Optimizer:  Adam\")\n",
        "print(\"  Loss:       Categorical Cross-Entropy\")\n",
        "print(\"  Metrics:    Accuracy\")\n",
        "\n",
        "# ============================================\n",
        "# PART C: Setup Callbacks\n",
        "# ============================================\n",
        "print(f\"\\n PART C: Setting up Training Callbacks...\\n\")\n",
        "\n",
        "# Early stopping - stop if no improvement\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Reduce learning rate on plateau\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=5,\n",
        "    min_lr=1e-7,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "callbacks = [early_stop, reduce_lr]\n",
        "\n",
        "print(\"‚úì Callbacks configured:\")\n",
        "print(\"  ‚Ä¢ Early Stopping (patience=10)\")\n",
        "print(\"  ‚Ä¢ Learning Rate Reduction (patience=5)\")\n",
        "\n",
        "# ============================================\n",
        "# PART D: Train Model\n",
        "# ============================================\n",
        "print(f\"\\n PART D: Training Model 1...\\n\")\n",
        "print(\"=\" * 70)\n",
        "print(\" Training in progress... (This may take 5-10 minutes)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Record training start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Train the model\n",
        "history_1 = model_1.fit(\n",
        "    X_train, y_train,\n",
        "    batch_size=64,\n",
        "    epochs=50,\n",
        "    validation_split=0.15,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Record training end time\n",
        "end_time = time.time()\n",
        "training_time_1 = end_time - start_time\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(f\"‚úì Training Complete!\")\n",
        "print(f\"  Total Time: {training_time_1/60:.2f} minutes\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ============================================\n",
        "# PART E: Evaluate Model\n",
        "# ============================================\n",
        "print(f\"\\n PART E: Evaluating Model on Test Set...\\n\")\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss_1, test_accuracy_1 = model_1.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üéØ MODEL 1 RESULTS:\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"  Test Loss:     {test_loss_1:.4f}\")\n",
        "print(f\"  Test Accuracy: {test_accuracy_1*100:.2f}%\")\n",
        "print(f\"  Training Time: {training_time_1/60:.2f} minutes\")\n",
        "print(f\"  Parameters:    {total_params:,}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ============================================\n",
        "# PART F: Plot Training History\n",
        "# ============================================\n",
        "print(f\"\\n PART F: Visualizing Training Progress...\\n\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "fig.suptitle('Model 1: Baseline Simple CNN - Training History',\n",
        "             fontsize=16, fontweight='bold')\n",
        "\n",
        "# Plot 1: Accuracy\n",
        "axes[0].plot(history_1.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
        "axes[0].plot(history_1.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
        "axes[0].set_title('Model Accuracy', fontsize=12, fontweight='bold')\n",
        "axes[0].set_xlabel('Epoch', fontsize=11)\n",
        "axes[0].set_ylabel('Accuracy', fontsize=11)\n",
        "axes[0].legend(loc='lower right')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Loss\n",
        "axes[1].plot(history_1.history['loss'], label='Training Loss', linewidth=2)\n",
        "axes[1].plot(history_1.history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "axes[1].set_title('Model Loss', fontsize=12, fontweight='bold')\n",
        "axes[1].set_xlabel('Epoch', fontsize=11)\n",
        "axes[1].set_ylabel('Loss', fontsize=11)\n",
        "axes[1].legend(loc='upper right')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================\n",
        "# PART G: Confusion Matrix\n",
        "# ============================================\n",
        "print(f\"\\n PART G: Generating Confusion Matrix...\\n\")\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "\n",
        "# Make predictions\n",
        "y_pred_1 = model_1.predict(X_test, verbose=0)\n",
        "y_pred_classes_1 = np.argmax(y_pred_1, axis=1)\n",
        "y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm_1 = confusion_matrix(y_true_classes, y_pred_classes_1)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(16, 14))\n",
        "sns.heatmap(cm_1, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES,\n",
        "            cbar_kws={'label': 'Count'})\n",
        "plt.title('Model 1: Confusion Matrix', fontsize=16, fontweight='bold', pad=20)\n",
        "plt.xlabel('Predicted Label', fontsize=12)\n",
        "plt.ylabel('True Label', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================\n",
        "# PART H: Per-Class Accuracy\n",
        "# ============================================\n",
        "print(f\"\\n PART H: Per-Character Accuracy Analysis...\\n\")\n",
        "\n",
        "# Calculate per-class accuracy\n",
        "class_accuracy_1 = []\n",
        "for i in range(NUM_CLASSES):\n",
        "    class_mask = (y_true_classes == i)\n",
        "    if np.sum(class_mask) > 0:\n",
        "        acc = np.sum((y_pred_classes_1 == i) & class_mask) / np.sum(class_mask)\n",
        "        class_accuracy_1.append(acc * 100)\n",
        "    else:\n",
        "        class_accuracy_1.append(0)\n",
        "\n",
        "# Create DataFrame for better visualization\n",
        "import pandas as pd\n",
        "accuracy_df_1 = pd.DataFrame({\n",
        "    'Character': CLASS_NAMES,\n",
        "    'Accuracy (%)': class_accuracy_1\n",
        "}).sort_values('Accuracy (%)', ascending=False)\n",
        "\n",
        "print(\"Top 10 Best Recognized Characters:\")\n",
        "print(\"-\" * 50)\n",
        "print(accuracy_df_1.head(10).to_string(index=False))\n",
        "\n",
        "print(\"\\nTop 10 Worst Recognized Characters:\")\n",
        "print(\"-\" * 50)\n",
        "print(accuracy_df_1.tail(10).to_string(index=False))\n",
        "\n",
        "# Plot per-class accuracy\n",
        "plt.figure(figsize=(16, 6))\n",
        "plt.bar(range(NUM_CLASSES), class_accuracy_1, color='steelblue', alpha=0.7)\n",
        "plt.axhline(y=test_accuracy_1*100, color='red', linestyle='--',\n",
        "            linewidth=2, label=f'Overall Accuracy: {test_accuracy_1*100:.2f}%')\n",
        "plt.xlabel('Character Index', fontsize=12)\n",
        "plt.ylabel('Accuracy (%)', fontsize=12)\n",
        "plt.title('Model 1: Per-Character Accuracy', fontsize=14, fontweight='bold')\n",
        "plt.xticks(range(NUM_CLASSES), CLASS_NAMES, rotation=90, fontsize=8)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3, axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================\n",
        "# PART I: Sample Predictions\n",
        "# ============================================\n",
        "print(f\"\\n PART I: Sample Predictions (Correct & Incorrect)...\\n\")\n",
        "\n",
        "# Find correct and incorrect predictions\n",
        "correct_idx = np.where(y_pred_classes_1 == y_true_classes)[0]\n",
        "incorrect_idx = np.where(y_pred_classes_1 != y_true_classes)[0]\n",
        "\n",
        "fig, axes = plt.subplots(2, 5, figsize=(18, 8))\n",
        "fig.suptitle('Model 1: Sample Predictions', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Show 5 correct predictions\n",
        "for i in range(5):\n",
        "    idx = correct_idx[i]\n",
        "    img = X_test[idx].reshape(IMG_HEIGHT, IMG_WIDTH)\n",
        "    true_label = CLASS_NAMES[y_true_classes[idx]]\n",
        "    pred_label = CLASS_NAMES[y_pred_classes_1[idx]]\n",
        "    confidence = np.max(y_pred_1[idx]) * 100\n",
        "\n",
        "    axes[0, i].imshow(img, cmap='gray')\n",
        "    axes[0, i].set_title(f\"‚úì True: '{true_label}'\\nPred: '{pred_label}'\\n{confidence:.1f}%\",\n",
        "                         fontsize=10, color='green')\n",
        "    axes[0, i].axis('off')\n",
        "\n",
        "# Show 5 incorrect predictions\n",
        "for i in range(5):\n",
        "    if i < len(incorrect_idx):\n",
        "        idx = incorrect_idx[i]\n",
        "        img = X_test[idx].reshape(IMG_HEIGHT, IMG_WIDTH)\n",
        "        true_label = CLASS_NAMES[y_true_classes[idx]]\n",
        "        pred_label = CLASS_NAMES[y_pred_classes_1[idx]]\n",
        "        confidence = np.max(y_pred_1[idx]) * 100\n",
        "\n",
        "        axes[1, i].imshow(img, cmap='gray')\n",
        "        axes[1, i].set_title(f\"‚úó True: '{true_label}'\\nPred: '{pred_label}'\\n{confidence:.1f}%\",\n",
        "                             fontsize=10, color='red')\n",
        "        axes[1, i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================\n",
        "# Summary\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\" MODEL 1 COMPLETE!\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"  Model Name:       Baseline Simple CNN\")\n",
        "print(f\"  Test Accuracy:    {test_accuracy_1*100:.2f}%\")\n",
        "print(f\"  Training Time:    {training_time_1/60:.2f} minutes\")\n",
        "print(f\"  Total Parameters: {total_params:,}\")\n",
        "print(f\"  Status:           ‚úì Trained and evaluated\")\n",
        "\n",
        "# Save results for later comparison\n",
        "model_1_results = {\n",
        "    'name': 'Baseline Simple CNN',\n",
        "    'accuracy': test_accuracy_1,\n",
        "    'loss': test_loss_1,\n",
        "    'params': total_params,\n",
        "    'time': training_time_1,\n",
        "    'history': history_1.history,\n",
        "    'predictions': y_pred_1,\n",
        "    'class_accuracy': class_accuracy_1\n",
        "}"
      ],
      "metadata": {
        "id": "WCji3Z49dfKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3k8wSuUrGyFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 9: MODEL 2 - IMPROVED DEEPER CNN\n",
        "\n",
        "# Deeper architecture with 4 convolutional layers\n",
        "# More parameters, better feature extraction\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\" MODEL 2: IMPROVED DEEPER CNN\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ============================================\n",
        "# PART A: Build Model Architecture\n",
        "# ============================================\n",
        "print(\"\\n  PART A: Building Model Architecture...\\n\")\n",
        "\n",
        "model_2 = Sequential([\n",
        "    # Input layer (implicit): 64x64x1\n",
        "\n",
        "    # Convolutional Block 1\n",
        "    Conv2D(32, kernel_size=(5, 5), activation='relu',\n",
        "           input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),\n",
        "           padding='same', name='conv1'),\n",
        "    MaxPooling2D(pool_size=(2, 2), name='pool1'),\n",
        "\n",
        "    # Convolutional Block 2\n",
        "    Conv2D(64, kernel_size=(3, 3), activation='relu',\n",
        "           padding='same', name='conv2'),\n",
        "    MaxPooling2D(pool_size=(2, 2), name='pool2'),\n",
        "    Dropout(0.3, name='dropout1'),\n",
        "\n",
        "    # Convolutional Block 3 (NEW - deeper!)\n",
        "    Conv2D(128, kernel_size=(3, 3), activation='relu',\n",
        "           padding='same', name='conv3'),\n",
        "    MaxPooling2D(pool_size=(2, 2), name='pool3'),\n",
        "\n",
        "    # Convolutional Block 4 (NEW - even deeper!)\n",
        "    Conv2D(256, kernel_size=(3, 3), activation='relu',\n",
        "           padding='same', name='conv4'),\n",
        "    MaxPooling2D(pool_size=(2, 2), name='pool4'),\n",
        "    Dropout(0.4, name='dropout2'),\n",
        "\n",
        "    # Fully Connected Layers\n",
        "    Flatten(name='flatten'),\n",
        "    Dense(256, activation='relu', name='dense1'),\n",
        "    Dropout(0.5, name='dropout3'),\n",
        "    Dense(128, activation='relu', name='dense2'),\n",
        "    Dropout(0.5, name='dropout4'),\n",
        "    Dense(NUM_CLASSES, activation='softmax', name='output')\n",
        "], name='Improved_Deeper_CNN')\n",
        "\n",
        "# Display architecture\n",
        "print(\" Model Architecture:\")\n",
        "print(\"-\" * 70)\n",
        "model_2.summary()\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Count parameters\n",
        "total_params_2 = model_2.count_params()\n",
        "print(f\"\\n Total Parameters: {total_params_2:,}\")\n",
        "print(f\"   Comparison to Model 1: {total_params_2/total_params:.2f}x more parameters\")\n",
        "\n",
        "# ============================================\n",
        "# PART B: Compile Model\n",
        "# ============================================\n",
        "print(f\"\\n  PART B: Compiling Model...\\n\")\n",
        "\n",
        "model_2.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"‚úì Model compiled successfully!\")\n",
        "print(\"  Optimizer:  Adam\")\n",
        "print(\"  Loss:       Categorical Cross-Entropy\")\n",
        "print(\"  Metrics:    Accuracy\")\n",
        "\n",
        "# ============================================\n",
        "# PART C: Setup Callbacks\n",
        "# ============================================\n",
        "print(f\"\\n PART C: Setting up Training Callbacks...\\n\")\n",
        "\n",
        "# Early stopping\n",
        "early_stop_2 = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Reduce learning rate\n",
        "reduce_lr_2 = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=5,\n",
        "    min_lr=1e-7,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "callbacks_2 = [early_stop_2, reduce_lr_2]\n",
        "\n",
        "print(\"‚úì Callbacks configured:\")\n",
        "print(\"  ‚Ä¢ Early Stopping (patience=10)\")\n",
        "print(\"  ‚Ä¢ Learning Rate Reduction (patience=5)\")\n",
        "\n",
        "# ============================================\n",
        "# PART D: Train Model\n",
        "# ============================================\n",
        "print(f\"\\n PART D: Training Model 2...\\n\")\n",
        "print(\"=\" * 70)\n",
        "print(\" Training in progress... (This may take 8-12 minutes)\")\n",
        "print(\"   Deeper model = longer training time\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Record training start time\n",
        "start_time_2 = time.time()\n",
        "\n",
        "# Train the model\n",
        "history_2 = model_2.fit(\n",
        "    X_train, y_train,\n",
        "    batch_size=64,\n",
        "    epochs=50,\n",
        "    validation_split=0.15,\n",
        "    callbacks=callbacks_2,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Record training end time\n",
        "end_time_2 = time.time()\n",
        "training_time_2 = end_time_2 - start_time_2\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(f\"‚úì Training Complete!\")\n",
        "print(f\"  Total Time: {training_time_2/60:.2f} minutes\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ============================================\n",
        "# PART E: Evaluate Model\n",
        "# ============================================\n",
        "print(f\"\\n PART E: Evaluating Model on Test Set...\\n\")\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss_2, test_accuracy_2 = model_2.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\" MODEL 2 RESULTS:\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"  Test Loss:     {test_loss_2:.4f}\")\n",
        "print(f\"  Test Accuracy: {test_accuracy_2*100:.2f}%\")\n",
        "print(f\"  Training Time: {training_time_2/60:.2f} minutes\")\n",
        "print(f\"  Parameters:    {total_params_2:,}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Compare with Model 1\n",
        "print(\"\\n COMPARISON WITH MODEL 1:\")\n",
        "print(\"=\" * 70)\n",
        "improvement = (test_accuracy_2 - test_accuracy_1) * 100\n",
        "time_increase = (training_time_2 - training_time_1) / 60\n",
        "\n",
        "print(f\"  Model 1 Accuracy: {test_accuracy_1*100:.2f}%\")\n",
        "print(f\"  Model 2 Accuracy: {test_accuracy_2*100:.2f}%\")\n",
        "if improvement > 0:\n",
        "    print(f\"  Improvement:      +{improvement:.2f}% ‚úì\")\n",
        "else:\n",
        "    print(f\"  Change:           {improvement:.2f}%\")\n",
        "print(f\"\\n  Model 1 Time:     {training_time_1/60:.2f} min\")\n",
        "print(f\"  Model 2 Time:     {training_time_2/60:.2f} min\")\n",
        "print(f\"  Time Increase:    +{time_increase:.2f} min\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ============================================\n",
        "# PART F: Plot Training History\n",
        "# ============================================\n",
        "print(f\"\\n PART F: Visualizing Training Progress...\\n\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "fig.suptitle('Model 2: Improved Deeper CNN - Training History',\n",
        "             fontsize=16, fontweight='bold')\n",
        "\n",
        "# Plot 1: Accuracy\n",
        "axes[0].plot(history_2.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
        "axes[0].plot(history_2.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
        "axes[0].set_title('Model Accuracy', fontsize=12, fontweight='bold')\n",
        "axes[0].set_xlabel('Epoch', fontsize=11)\n",
        "axes[0].set_ylabel('Accuracy', fontsize=11)\n",
        "axes[0].legend(loc='lower right')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Loss\n",
        "axes[1].plot(history_2.history['loss'], label='Training Loss', linewidth=2)\n",
        "axes[1].plot(history_2.history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "axes[1].set_title('Model Loss', fontsize=12, fontweight='bold')\n",
        "axes[1].set_xlabel('Epoch', fontsize=11)\n",
        "axes[1].set_ylabel('Loss', fontsize=11)\n",
        "axes[1].legend(loc='upper right')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================\n",
        "# PART G: Compare Training Curves (Model 1 vs Model 2)\n",
        "# ============================================\n",
        "print(f\"\\n PART G: Comparing Training Curves...\\n\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "fig.suptitle('Model 1 vs Model 2: Training Comparison',\n",
        "             fontsize=16, fontweight='bold')\n",
        "\n",
        "# Plot 1: Validation Accuracy Comparison\n",
        "axes[0].plot(history_1.history['val_accuracy'], label='Model 1 (Baseline)',\n",
        "             linewidth=2, linestyle='--', alpha=0.8)\n",
        "axes[0].plot(history_2.history['val_accuracy'], label='Model 2 (Improved)',\n",
        "             linewidth=2, alpha=0.8)\n",
        "axes[0].set_title('Validation Accuracy Comparison', fontsize=12, fontweight='bold')\n",
        "axes[0].set_xlabel('Epoch', fontsize=11)\n",
        "axes[0].set_ylabel('Accuracy', fontsize=11)\n",
        "axes[0].legend(loc='lower right')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Validation Loss Comparison\n",
        "axes[1].plot(history_1.history['val_loss'], label='Model 1 (Baseline)',\n",
        "             linewidth=2, linestyle='--', alpha=0.8)\n",
        "axes[1].plot(history_2.history['val_loss'], label='Model 2 (Improved)',\n",
        "             linewidth=2, alpha=0.8)\n",
        "axes[1].set_title('Validation Loss Comparison', fontsize=12, fontweight='bold')\n",
        "axes[1].set_xlabel('Epoch', fontsize=11)\n",
        "axes[1].set_ylabel('Loss', fontsize=11)\n",
        "axes[1].legend(loc='upper right')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================\n",
        "# PART H: Confusion Matrix\n",
        "# ============================================\n",
        "print(f\"\\n PART H: Generating Confusion Matrix...\\n\")\n",
        "\n",
        "# Make predictions\n",
        "y_pred_2 = model_2.predict(X_test, verbose=0)\n",
        "y_pred_classes_2 = np.argmax(y_pred_2, axis=1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm_2 = confusion_matrix(y_true_classes, y_pred_classes_2)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(16, 14))\n",
        "sns.heatmap(cm_2, annot=True, fmt='d', cmap='Greens',\n",
        "            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES,\n",
        "            cbar_kws={'label': 'Count'})\n",
        "plt.title('Model 2: Confusion Matrix', fontsize=16, fontweight='bold', pad=20)\n",
        "plt.xlabel('Predicted Label', fontsize=12)\n",
        "plt.ylabel('True Label', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================\n",
        "# PART I: Per-Class Accuracy\n",
        "# ============================================\n",
        "print(f\"\\n PART I: Per-Character Accuracy Analysis...\\n\")\n",
        "\n",
        "# Calculate per-class accuracy\n",
        "class_accuracy_2 = []\n",
        "for i in range(NUM_CLASSES):\n",
        "    class_mask = (y_true_classes == i)\n",
        "    if np.sum(class_mask) > 0:\n",
        "        acc = np.sum((y_pred_classes_2 == i) & class_mask) / np.sum(class_mask)\n",
        "        class_accuracy_2.append(acc * 100)\n",
        "    else:\n",
        "        class_accuracy_2.append(0)\n",
        "\n",
        "# Create DataFrame\n",
        "accuracy_df_2 = pd.DataFrame({\n",
        "    'Character': CLASS_NAMES,\n",
        "    'Model 1 (%)': class_accuracy_1,\n",
        "    'Model 2 (%)': class_accuracy_2,\n",
        "    'Improvement': [m2 - m1 for m1, m2 in zip(class_accuracy_1, class_accuracy_2)]\n",
        "}).sort_values('Model 2 (%)', ascending=False)\n",
        "\n",
        "print(\"Top 10 Best Recognized Characters (Model 2):\")\n",
        "print(\"-\" * 70)\n",
        "print(accuracy_df_2.head(10).to_string(index=False))\n",
        "\n",
        "print(\"\\nTop 10 Worst Recognized Characters (Model 2):\")\n",
        "print(\"-\" * 70)\n",
        "print(accuracy_df_2.tail(10).to_string(index=False))\n",
        "\n",
        "print(\"\\nCharacters with Biggest Improvement:\")\n",
        "print(\"-\" * 70)\n",
        "print(accuracy_df_2.nlargest(5, 'Improvement')[['Character', 'Model 1 (%)', 'Model 2 (%)', 'Improvement']].to_string(index=False))\n",
        "\n",
        "# Plot comparison\n",
        "fig, ax = plt.subplots(figsize=(16, 6))\n",
        "x = np.arange(NUM_CLASSES)\n",
        "width = 0.35\n",
        "\n",
        "bars1 = ax.bar(x - width/2, class_accuracy_1, width, label='Model 1', alpha=0.7, color='steelblue')\n",
        "bars2 = ax.bar(x + width/2, class_accuracy_2, width, label='Model 2', alpha=0.7, color='seagreen')\n",
        "\n",
        "ax.axhline(y=test_accuracy_1*100, color='blue', linestyle='--', linewidth=1, alpha=0.5)\n",
        "ax.axhline(y=test_accuracy_2*100, color='green', linestyle='--', linewidth=1, alpha=0.5)\n",
        "\n",
        "ax.set_xlabel('Character', fontsize=12)\n",
        "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
        "ax.set_title('Per-Character Accuracy: Model 1 vs Model 2', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(CLASS_NAMES, rotation=90, fontsize=8)\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================\n",
        "# PART J: Sample Predictions\n",
        "# ============================================\n",
        "print(f\"\\n PART J: Sample Predictions...\\n\")\n",
        "\n",
        "# Find correct and incorrect predictions\n",
        "correct_idx_2 = np.where(y_pred_classes_2 == y_true_classes)[0]\n",
        "incorrect_idx_2 = np.where(y_pred_classes_2 != y_true_classes)[0]\n",
        "\n",
        "fig, axes = plt.subplots(2, 5, figsize=(18, 8))\n",
        "fig.suptitle('Model 2: Sample Predictions', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Show 5 correct predictions\n",
        "for i in range(5):\n",
        "    idx = correct_idx_2[i]\n",
        "    img = X_test[idx].reshape(IMG_HEIGHT, IMG_WIDTH)\n",
        "    true_label = CLASS_NAMES[y_true_classes[idx]]\n",
        "    pred_label = CLASS_NAMES[y_pred_classes_2[idx]]\n",
        "    confidence = np.max(y_pred_2[idx]) * 100\n",
        "\n",
        "    axes[0, i].imshow(img, cmap='gray')\n",
        "    axes[0, i].set_title(f\"‚úì True: '{true_label}'\\nPred: '{pred_label}'\\n{confidence:.1f}%\",\n",
        "                         fontsize=10, color='green')\n",
        "    axes[0, i].axis('off')\n",
        "\n",
        "# Show 5 incorrect predictions\n",
        "for i in range(5):\n",
        "    if i < len(incorrect_idx_2):\n",
        "        idx = incorrect_idx_2[i]\n",
        "        img = X_test[idx].reshape(IMG_HEIGHT, IMG_WIDTH)\n",
        "        true_label = CLASS_NAMES[y_true_classes[idx]]\n",
        "        pred_label = CLASS_NAMES[y_pred_classes_2[idx]]\n",
        "        confidence = np.max(y_pred_2[idx]) * 100\n",
        "\n",
        "        axes[1, i].imshow(img, cmap='gray')\n",
        "        axes[1, i].set_title(f\"‚úó True: '{true_label}'\\nPred: '{pred_label}'\\n{confidence:.1f}%\",\n",
        "                             fontsize=10, color='red')\n",
        "        axes[1, i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================\n",
        "# Summary\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\" MODEL 2 COMPLETE!\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"  Model Name:       Improved Deeper CNN\")\n",
        "print(f\"  Test Accuracy:    {test_accuracy_2*100:.2f}%\")\n",
        "print(f\"  Training Time:    {training_time_2/60:.2f} minutes\")\n",
        "print(f\"  Total Parameters: {total_params_2:,}\")\n",
        "print(f\"  Status:           ‚úì Trained and evaluated\")\n",
        "print(\"\\n   Improvement over Model 1:\")\n",
        "if improvement > 0:\n",
        "    print(f\"     Accuracy gain: +{improvement:.2f}% ‚úì\")\n",
        "else:\n",
        "    print(f\"     Accuracy:      {improvement:.2f}%\")\n",
        "\n",
        "# Save results for later comparison\n",
        "model_2_results = {\n",
        "    'name': 'Improved Deeper CNN',\n",
        "    'accuracy': test_accuracy_2,\n",
        "    'loss': test_loss_2,\n",
        "    'params': total_params_2,\n",
        "    'time': training_time_2,\n",
        "    'history': history_2.history,\n",
        "    'predictions': y_pred_2,\n",
        "    'class_accuracy': class_accuracy_2\n",
        "}"
      ],
      "metadata": {
        "id": "ZwuGJ1FLeL7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 10: MODEL 3 - ADVANCED RESNET-INSPIRED CNN\n",
        "\n",
        "# ResNet-inspired architecture with residual connections\n",
        "# State-of-the-art performance with skip connections\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\" MODEL 3: ADVANCED RESNET-INSPIRED CNN\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Add, BatchNormalization, GlobalAveragePooling2D\n",
        "\n",
        "# ============================================\n",
        "# PART A: Build Model Architecture with Residual Blocks\n",
        "# ============================================\n",
        "print(\"\\n  PART A: Building ResNet-Inspired Architecture...\\n\")\n",
        "\n",
        "# Define residual block function\n",
        "def residual_block(x, filters, kernel_size=3, name_prefix='res'):\n",
        "    \"\"\"\n",
        "    Creates a residual block with skip connection\n",
        "    \"\"\"\n",
        "    # Store input for skip connection\n",
        "    shortcut = x\n",
        "\n",
        "    # First convolution\n",
        "    x = Conv2D(filters, kernel_size, padding='same',\n",
        "               name=f'{name_prefix}_conv1')(x)\n",
        "    x = BatchNormalization(name=f'{name_prefix}_bn1')(x)\n",
        "    x = tf.keras.layers.Activation('relu', name=f'{name_prefix}_relu1')(x)\n",
        "\n",
        "    # Second convolution\n",
        "    x = Conv2D(filters, kernel_size, padding='same',\n",
        "               name=f'{name_prefix}_conv2')(x)\n",
        "    x = BatchNormalization(name=f'{name_prefix}_bn2')(x)\n",
        "\n",
        "    # Adjust shortcut if needed\n",
        "    if shortcut.shape[-1] != filters:\n",
        "        shortcut = Conv2D(filters, 1, padding='same',\n",
        "                         name=f'{name_prefix}_shortcut')(shortcut)\n",
        "\n",
        "    # Add skip connection\n",
        "    x = Add(name=f'{name_prefix}_add')([x, shortcut])\n",
        "    x = tf.keras.layers.Activation('relu', name=f'{name_prefix}_relu2')(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "# Build the model using Functional API\n",
        "print(\"Building model with residual connections...\")\n",
        "\n",
        "# Input layer\n",
        "inputs = Input(shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), name='input')\n",
        "\n",
        "# Initial convolution\n",
        "x = Conv2D(64, 7, strides=2, padding='same', name='initial_conv')(inputs)\n",
        "x = BatchNormalization(name='initial_bn')(x)\n",
        "x = tf.keras.layers.Activation('relu', name='initial_relu')(x)\n",
        "x = MaxPooling2D(3, strides=2, padding='same', name='initial_pool')(x)\n",
        "\n",
        "# Residual Block 1 (64 filters)\n",
        "x = residual_block(x, 64, name_prefix='res1_block1')\n",
        "x = residual_block(x, 64, name_prefix='res1_block2')\n",
        "\n",
        "# Transition with pooling\n",
        "x = MaxPooling2D(2, name='pool1')(x)\n",
        "x = Dropout(0.3, name='dropout1')(x)\n",
        "\n",
        "# Residual Block 2 (128 filters)\n",
        "x = residual_block(x, 128, name_prefix='res2_block1')\n",
        "x = residual_block(x, 128, name_prefix='res2_block2')\n",
        "\n",
        "# Transition with pooling\n",
        "x = MaxPooling2D(2, name='pool2')(x)\n",
        "x = Dropout(0.4, name='dropout2')(x)\n",
        "\n",
        "# Residual Block 3 (256 filters)\n",
        "x = residual_block(x, 256, name_prefix='res3_block1')\n",
        "x = residual_block(x, 256, name_prefix='res3_block2')\n",
        "\n",
        "# Global Average Pooling (instead of Flatten)\n",
        "x = GlobalAveragePooling2D(name='global_avg_pool')(x)\n",
        "\n",
        "# Fully Connected Layers\n",
        "x = Dense(512, activation='relu', name='dense1')(x)\n",
        "x = Dropout(0.5, name='dropout3')(x)\n",
        "x = Dense(256, activation='relu', name='dense2')(x)\n",
        "x = Dropout(0.5, name='dropout4')(x)\n",
        "\n",
        "# Output layer\n",
        "outputs = Dense(NUM_CLASSES, activation='softmax', name='output')(x)\n",
        "\n",
        "# Create model\n",
        "model_3 = Model(inputs=inputs, outputs=outputs, name='ResNet_Inspired_CNN')\n",
        "\n",
        "# Display architecture\n",
        "print(\"\\n Model Architecture:\")\n",
        "print(\"-\" * 70)\n",
        "model_3.summary()\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Count parameters\n",
        "total_params_3 = model_3.count_params()\n",
        "print(f\"\\n Total Parameters: {total_params_3:,}\")\n",
        "print(f\"   Comparison to Model 1: {total_params_3/total_params:.2f}x\")\n",
        "print(f\"   Comparison to Model 2: {total_params_3/total_params_2:.2f}x\")\n",
        "\n",
        "print(\"\\n Key Features of ResNet Architecture:\")\n",
        "print(\"   ‚úì Residual (skip) connections - helps gradient flow\")\n",
        "print(\"   ‚úì Batch Normalization - faster & more stable training\")\n",
        "print(\"   ‚úì Global Average Pooling - reduces overfitting\")\n",
        "print(\"   ‚úì Deeper network (6 residual blocks) - better feature learning\")\n",
        "\n",
        "# ============================================\n",
        "# PART B: Compile Model\n",
        "# ============================================\n",
        "print(f\"\\n  PART B: Compiling Model...\\n\")\n",
        "\n",
        "model_3.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"‚úì Model compiled successfully!\")\n",
        "print(\"  Optimizer:  Adam\")\n",
        "print(\"  Loss:       Categorical Cross-Entropy\")\n",
        "print(\"  Metrics:    Accuracy\")\n",
        "\n",
        "# ============================================\n",
        "# PART C: Setup Callbacks\n",
        "# ============================================\n",
        "print(f\"\\n PART C: Setting up Training Callbacks...\\n\")\n",
        "\n",
        "# Early stopping\n",
        "early_stop_3 = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=15,  # More patience for complex model\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Reduce learning rate\n",
        "reduce_lr_3 = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=7,\n",
        "    min_lr=1e-7,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "callbacks_3 = [early_stop_3, reduce_lr_3]\n",
        "\n",
        "print(\"‚úì Callbacks configured:\")\n",
        "print(\"  ‚Ä¢ Early Stopping (patience=15)\")\n",
        "print(\"  ‚Ä¢ Learning Rate Reduction (patience=7)\")\n",
        "print(\"  ‚Ä¢ Higher patience for deeper model\")\n",
        "\n",
        "# ============================================\n",
        "# PART D: Train Model\n",
        "# ============================================\n",
        "print(f\"\\n PART D: Training Model 3...\\n\")\n",
        "print(\"=\" * 70)\n",
        "print(\" Training in progress... (This may take 10-15 minutes)\")\n",
        "print(\"   Advanced architecture = longer but better training\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Record training start time\n",
        "start_time_3 = time.time()\n",
        "\n",
        "# Train the model\n",
        "history_3 = model_3.fit(\n",
        "    X_train, y_train,\n",
        "    batch_size=64,\n",
        "    epochs=50,\n",
        "    validation_split=0.15,\n",
        "    callbacks=callbacks_3,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Record training end time\n",
        "end_time_3 = time.time()\n",
        "training_time_3 = end_time_3 - start_time_3\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(f\"‚úì Training Complete!\")\n",
        "print(f\"  Total Time: {training_time_3/60:.2f} minutes\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ============================================\n",
        "# PART E: Evaluate Model\n",
        "# ============================================\n",
        "print(f\"\\n PART E: Evaluating Model on Test Set...\\n\")\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss_3, test_accuracy_3 = model_3.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\" MODEL 3 RESULTS:\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"  Test Loss:     {test_loss_3:.4f}\")\n",
        "print(f\"  Test Accuracy: {test_accuracy_3*100:.2f}%\")\n",
        "print(f\"  Training Time: {training_time_3/60:.2f} minutes\")\n",
        "print(f\"  Parameters:    {total_params_3:,}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Compare with previous models\n",
        "print(\"\\n COMPARISON WITH ALL MODELS:\")\n",
        "print(\"=\" * 70)\n",
        "improvement_vs_1 = (test_accuracy_3 - test_accuracy_1) * 100\n",
        "improvement_vs_2 = (test_accuracy_3 - test_accuracy_2) * 100\n",
        "\n",
        "print(f\"  Model 1 Accuracy: {test_accuracy_1*100:.2f}%\")\n",
        "print(f\"  Model 2 Accuracy: {test_accuracy_2*100:.2f}%\")\n",
        "print(f\"  Model 3 Accuracy: {test_accuracy_3*100:.2f}%\")\n",
        "print()\n",
        "if improvement_vs_1 > 0:\n",
        "    print(f\"  vs Model 1:       +{improvement_vs_1:.2f}% ‚úì\")\n",
        "else:\n",
        "    print(f\"  vs Model 1:       {improvement_vs_1:.2f}%\")\n",
        "\n",
        "if improvement_vs_2 > 0:\n",
        "    print(f\"  vs Model 2:       +{improvement_vs_2:.2f}% ‚úì\")\n",
        "else:\n",
        "    print(f\"  vs Model 2:       {improvement_vs_2:.2f}%\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ============================================\n",
        "# PART F: Plot Training History\n",
        "# ============================================\n",
        "print(f\"\\nüìà PART F: Visualizing Training Progress...\\n\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "fig.suptitle('Model 3: ResNet-Inspired CNN - Training History',\n",
        "             fontsize=16, fontweight='bold')\n",
        "\n",
        "# Plot 1: Accuracy\n",
        "axes[0].plot(history_3.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
        "axes[0].plot(history_3.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
        "axes[0].set_title('Model Accuracy', fontsize=12, fontweight='bold')\n",
        "axes[0].set_xlabel('Epoch', fontsize=11)\n",
        "axes[0].set_ylabel('Accuracy', fontsize=11)\n",
        "axes[0].legend(loc='lower right')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Loss\n",
        "axes[1].plot(history_3.history['loss'], label='Training Loss', linewidth=2)\n",
        "axes[1].plot(history_3.history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "axes[1].set_title('Model Loss', fontsize=12, fontweight='bold')\n",
        "axes[1].set_xlabel('Epoch', fontsize=11)\n",
        "axes[1].set_ylabel('Loss', fontsize=11)\n",
        "axes[1].legend(loc='upper right')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================\n",
        "# PART G: Compare ALL Training Curves\n",
        "# ============================================\n",
        "print(f\"\\n PART G: Comparing All 3 Models...\\n\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "fig.suptitle('All Models Comparison: Training Performance',\n",
        "             fontsize=16, fontweight='bold')\n",
        "\n",
        "# Plot 1: Validation Accuracy\n",
        "axes[0].plot(history_1.history['val_accuracy'], label='Model 1 (Baseline)',\n",
        "             linewidth=2, linestyle='--', alpha=0.7)\n",
        "axes[0].plot(history_2.history['val_accuracy'], label='Model 2 (Improved)',\n",
        "             linewidth=2, linestyle='-.', alpha=0.7)\n",
        "axes[0].plot(history_3.history['val_accuracy'], label='Model 3 (ResNet)',\n",
        "             linewidth=2.5, alpha=0.9)\n",
        "axes[0].set_title('Validation Accuracy Comparison', fontsize=12, fontweight='bold')\n",
        "axes[0].set_xlabel('Epoch', fontsize=11)\n",
        "axes[0].set_ylabel('Accuracy', fontsize=11)\n",
        "axes[0].legend(loc='lower right')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Validation Loss\n",
        "axes[1].plot(history_1.history['val_loss'], label='Model 1 (Baseline)',\n",
        "             linewidth=2, linestyle='--', alpha=0.7)\n",
        "axes[1].plot(history_2.history['val_loss'], label='Model 2 (Improved)',\n",
        "             linewidth=2, linestyle='-.', alpha=0.7)\n",
        "axes[1].plot(history_3.history['val_loss'], label='Model 3 (ResNet)',\n",
        "             linewidth=2.5, alpha=0.9)\n",
        "axes[1].set_title('Validation Loss Comparison', fontsize=12, fontweight='bold')\n",
        "axes[1].set_xlabel('Epoch', fontsize=11)\n",
        "axes[1].set_ylabel('Loss', fontsize=11)\n",
        "axes[1].legend(loc='upper right')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================\n",
        "# PART H: Confusion Matrix\n",
        "# ============================================\n",
        "print(f\"\\n PART H: Generating Confusion Matrix...\\n\")\n",
        "\n",
        "# Make predictions\n",
        "y_pred_3 = model_3.predict(X_test, verbose=0)\n",
        "y_pred_classes_3 = np.argmax(y_pred_3, axis=1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm_3 = confusion_matrix(y_true_classes, y_pred_classes_3)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(16, 14))\n",
        "sns.heatmap(cm_3, annot=True, fmt='d', cmap='Purples',\n",
        "            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES,\n",
        "            cbar_kws={'label': 'Count'})\n",
        "plt.title('Model 3: Confusion Matrix', fontsize=16, fontweight='bold', pad=20)\n",
        "plt.xlabel('Predicted Label', fontsize=12)\n",
        "plt.ylabel('True Label', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================\n",
        "# PART I: Per-Class Accuracy Analysis\n",
        "# ============================================\n",
        "print(f\"\\n PART I: Per-Character Accuracy Analysis...\\n\")\n",
        "\n",
        "# Calculate per-class accuracy for Model 3\n",
        "class_accuracy_3 = []\n",
        "for i in range(NUM_CLASSES):\n",
        "    class_mask = (y_true_classes == i)\n",
        "    if np.sum(class_mask) > 0:\n",
        "        acc = np.sum((y_pred_classes_3 == i) & class_mask) / np.sum(class_mask)\n",
        "        class_accuracy_3.append(acc * 100)\n",
        "    else:\n",
        "        class_accuracy_3.append(0)\n",
        "\n",
        "# Create comprehensive DataFrame\n",
        "accuracy_comparison_df = pd.DataFrame({\n",
        "    'Character': CLASS_NAMES,\n",
        "    'Model 1 (%)': class_accuracy_1,\n",
        "    'Model 2 (%)': class_accuracy_2,\n",
        "    'Model 3 (%)': class_accuracy_3,\n",
        "    'Best Improvement': [max(m1, m2, m3) - m1\n",
        "                         for m1, m2, m3 in zip(class_accuracy_1,\n",
        "                                               class_accuracy_2,\n",
        "                                               class_accuracy_3)]\n",
        "}).sort_values('Model 3 (%)', ascending=False)\n",
        "\n",
        "print(\"Top 10 Best Recognized Characters (Model 3):\")\n",
        "print(\"-\" * 80)\n",
        "print(accuracy_comparison_df.head(10).to_string(index=False))\n",
        "\n",
        "print(\"\\nTop 10 Worst Recognized Characters (Model 3):\")\n",
        "print(\"-\" * 80)\n",
        "print(accuracy_comparison_df.tail(10).to_string(index=False))\n",
        "\n",
        "print(\"\\nCharacters with Biggest Overall Improvement:\")\n",
        "print(\"-\" * 80)\n",
        "print(accuracy_comparison_df.nlargest(5, 'Best Improvement')[\n",
        "    ['Character', 'Model 1 (%)', 'Model 3 (%)', 'Best Improvement']\n",
        "].to_string(index=False))\n",
        "\n",
        "# Plot 3-way comparison\n",
        "fig, ax = plt.subplots(figsize=(18, 7))\n",
        "x = np.arange(NUM_CLASSES)\n",
        "width = 0.25\n",
        "\n",
        "bars1 = ax.bar(x - width, class_accuracy_1, width, label='Model 1', alpha=0.7, color='steelblue')\n",
        "bars2 = ax.bar(x, class_accuracy_2, width, label='Model 2', alpha=0.7, color='seagreen')\n",
        "bars3 = ax.bar(x + width, class_accuracy_3, width, label='Model 3', alpha=0.7, color='purple')\n",
        "\n",
        "ax.axhline(y=test_accuracy_1*100, color='blue', linestyle='--', linewidth=1, alpha=0.3)\n",
        "ax.axhline(y=test_accuracy_2*100, color='green', linestyle='--', linewidth=1, alpha=0.3)\n",
        "ax.axhline(y=test_accuracy_3*100, color='purple', linestyle='--', linewidth=1, alpha=0.3)\n",
        "\n",
        "ax.set_xlabel('Character', fontsize=12)\n",
        "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
        "ax.set_title('Per-Character Accuracy: All 3 Models Comparison', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(CLASS_NAMES, rotation=90, fontsize=8)\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================\n",
        "# PART J: Sample Predictions\n",
        "# ============================================\n",
        "print(f\"\\n PART J: Sample Predictions...\\n\")\n",
        "\n",
        "# Find correct and incorrect predictions\n",
        "correct_idx_3 = np.where(y_pred_classes_3 == y_true_classes)[0]\n",
        "incorrect_idx_3 = np.where(y_pred_classes_3 != y_true_classes)[0]\n",
        "\n",
        "fig, axes = plt.subplots(2, 5, figsize=(18, 8))\n",
        "fig.suptitle('Model 3: Sample Predictions', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Show 5 correct predictions\n",
        "for i in range(5):\n",
        "    idx = correct_idx_3[i]\n",
        "    img = X_test[idx].reshape(IMG_HEIGHT, IMG_WIDTH)\n",
        "    true_label = CLASS_NAMES[y_true_classes[idx]]\n",
        "    pred_label = CLASS_NAMES[y_pred_classes_3[idx]]\n",
        "    confidence = np.max(y_pred_3[idx]) * 100\n",
        "\n",
        "    axes[0, i].imshow(img, cmap='gray')\n",
        "    axes[0, i].set_title(f\"‚úì True: '{true_label}'\\nPred: '{pred_label}'\\n{confidence:.1f}%\",\n",
        "                         fontsize=10, color='green')\n",
        "    axes[0, i].axis('off')\n",
        "\n",
        "# Show 5 incorrect predictions\n",
        "for i in range(min(5, len(incorrect_idx_3))):\n",
        "    idx = incorrect_idx_3[i]\n",
        "    img = X_test[idx].reshape(IMG_HEIGHT, IMG_WIDTH)\n",
        "    true_label = CLASS_NAMES[y_true_classes[idx]]\n",
        "    pred_label = CLASS_NAMES[y_pred_classes_3[idx]]\n",
        "    confidence = np.max(y_pred_3[idx]) * 100\n",
        "\n",
        "    axes[1, i].imshow(img, cmap='gray')\n",
        "    axes[1, i].set_title(f\"‚úó True: '{true_label}'\\nPred: '{pred_label}'\\n{confidence:.1f}%\",\n",
        "                         fontsize=10, color='red')\n",
        "    axes[1, i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================\n",
        "# Summary\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\" MODEL 3 COMPLETE!\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"  Model Name:       Advanced ResNet-Inspired CNN\")\n",
        "print(f\"  Test Accuracy:    {test_accuracy_3*100:.2f}%\")\n",
        "print(f\"  Training Time:    {training_time_3/60:.2f} minutes\")\n",
        "print(f\"  Total Parameters: {total_params_3:,}\")\n",
        "print(f\"  Status:           ‚úì Trained and evaluated\")\n",
        "print(\"\\n   Performance Summary:\")\n",
        "print(f\"     Model 1: {test_accuracy_1*100:.2f}%\")\n",
        "print(f\"     Model 2: {test_accuracy_2*100:.2f}%\")\n",
        "print(f\"     Model 3: {test_accuracy_3*100:.2f}% \")\n",
        "if test_accuracy_3 >= test_accuracy_2 and test_accuracy_3 >= test_accuracy_1:\n",
        "    print(f\"\\n      Model 3 is the BEST performer!\")\n",
        "\n",
        "\n",
        "# Save results for final comparison\n",
        "model_3_results = {\n",
        "    'name': 'Advanced ResNet-Inspired CNN',\n",
        "    'accuracy': test_accuracy_3,\n",
        "    'loss': test_loss_3,\n",
        "    'params': total_params_3,\n",
        "    'time': training_time_3,\n",
        "    'history': history_3.history,\n",
        "    'predictions': y_pred_3,\n",
        "    'class_accuracy': class_accuracy_3\n",
        "}"
      ],
      "metadata": {
        "id": "kK7CQKV7fAko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 11: FINAL COMPARISON & ANALYSIS\n",
        "\n",
        "# Comprehensive comparison of all 3 models\n",
        "# Performance analysis and recommendations\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\" FINAL MODEL COMPARISON & ANALYSIS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ============================================\n",
        "# PART A: Summary Table\n",
        "# ============================================\n",
        "print(\"\\n PART A: Performance Summary Table\\n\")\n",
        "\n",
        "# Create comprehensive comparison DataFrame\n",
        "comparison_data = {\n",
        "    'Model': [\n",
        "        'Model 1: Baseline Simple CNN',\n",
        "        'Model 2: Improved Deeper CNN',\n",
        "        'Model 3: Advanced ResNet-Inspired'\n",
        "    ],\n",
        "    'Architecture': [\n",
        "        '2 Conv Blocks',\n",
        "        '4 Conv Blocks',\n",
        "        '6 Residual Blocks'\n",
        "    ],\n",
        "    'Parameters': [\n",
        "        f'{total_params:,}',\n",
        "        f'{total_params_2:,}',\n",
        "        f'{total_params_3:,}'\n",
        "    ],\n",
        "    'Test Accuracy (%)': [\n",
        "        f'{test_accuracy_1*100:.2f}',\n",
        "        f'{test_accuracy_2*100:.2f}',\n",
        "        f'{test_accuracy_3*100:.2f}'\n",
        "    ],\n",
        "    'Test Loss': [\n",
        "        f'{test_loss_1:.4f}',\n",
        "        f'{test_loss_2:.4f}',\n",
        "        f'{test_loss_3:.4f}'\n",
        "    ],\n",
        "    'Training Time (min)': [\n",
        "        f'{training_time_1/60:.2f}',\n",
        "        f'{training_time_2/60:.2f}',\n",
        "        f'{training_time_3/60:.2f}'\n",
        "    ]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "\n",
        "print(\"=\" * 100)\n",
        "print(comparison_df.to_string(index=False))\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Find best model\n",
        "accuracies = [test_accuracy_1, test_accuracy_2, test_accuracy_3]\n",
        "best_model_idx = np.argmax(accuracies)\n",
        "model_names = ['Model 1', 'Model 2', 'Model 3']\n",
        "\n",
        "print(f\"\\n BEST MODEL: {model_names[best_model_idx]}\")\n",
        "print(f\"   Best Accuracy: {accuracies[best_model_idx]*100:.2f}%\")\n",
        "\n",
        "# ============================================\n",
        "# PART B: Accuracy Comparison Visualization\n",
        "# ============================================\n",
        "print(f\"\\n PART B: Visualizing Model Comparison...\\n\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "fig.suptitle('Comprehensive Model Comparison', fontsize=18, fontweight='bold')\n",
        "\n",
        "# Plot 1: Accuracy Bar Chart\n",
        "ax1 = axes[0, 0]\n",
        "models = ['Model 1\\n(Baseline)', 'Model 2\\n(Improved)', 'Model 3\\n(ResNet)']\n",
        "accuracies_percent = [test_accuracy_1*100, test_accuracy_2*100, test_accuracy_3*100]\n",
        "colors = ['steelblue', 'seagreen', 'purple']\n",
        "\n",
        "bars = ax1.bar(models, accuracies_percent, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
        "ax1.set_ylabel('Test Accuracy (%)', fontsize=12, fontweight='bold')\n",
        "ax1.set_title('Test Accuracy Comparison', fontsize=14, fontweight='bold')\n",
        "ax1.set_ylim([85, 100])\n",
        "ax1.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels on bars\n",
        "for i, (bar, acc) in enumerate(zip(bars, accuracies_percent)):\n",
        "    height = bar.get_height()\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
        "             f'{acc:.2f}%',\n",
        "             ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
        "\n",
        "    # Highlight best model\n",
        "    if i == best_model_idx:\n",
        "        bar.set_edgecolor('gold')\n",
        "        bar.set_linewidth(4)\n",
        "\n",
        "# Plot 2: Training Time vs Accuracy\n",
        "ax2 = axes[0, 1]\n",
        "times = [training_time_1/60, training_time_2/60, training_time_3/60]\n",
        "params_size = [total_params/1e6, total_params_2/1e6, total_params_3/1e6]\n",
        "\n",
        "scatter = ax2.scatter(times, accuracies_percent,\n",
        "                     s=[p*500 for p in params_size],  # Size based on parameters\n",
        "                     c=colors, alpha=0.6, edgecolor='black', linewidth=2)\n",
        "\n",
        "for i, model in enumerate(['M1', 'M2', 'M3']):\n",
        "    ax2.annotate(model, (times[i], accuracies_percent[i]),\n",
        "                fontsize=12, fontweight='bold', ha='center', va='center')\n",
        "\n",
        "ax2.set_xlabel('Training Time (minutes)', fontsize=12, fontweight='bold')\n",
        "ax2.set_ylabel('Test Accuracy (%)', fontsize=12, fontweight='bold')\n",
        "ax2.set_title('Efficiency Analysis\\n(Bubble size = Model complexity)',\n",
        "             fontsize=14, fontweight='bold')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Parameters vs Accuracy\n",
        "ax3 = axes[1, 0]\n",
        "params_millions = [total_params/1e6, total_params_2/1e6, total_params_3/1e6]\n",
        "\n",
        "bars2 = ax3.barh(models, params_millions, color=colors, alpha=0.7,\n",
        "                 edgecolor='black', linewidth=2)\n",
        "ax3.set_xlabel('Parameters (Millions)', fontsize=12, fontweight='bold')\n",
        "ax3.set_title('Model Complexity (Parameter Count)', fontsize=14, fontweight='bold')\n",
        "ax3.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Add value labels\n",
        "for bar, param in zip(bars2, params_millions):\n",
        "    width = bar.get_width()\n",
        "    ax3.text(width, bar.get_y() + bar.get_height()/2.,\n",
        "             f'{param:.2f}M',\n",
        "             ha='left', va='center', fontsize=11, fontweight='bold')\n",
        "\n",
        "# Plot 4: Training Curves Comparison\n",
        "ax4 = axes[1, 1]\n",
        "epochs_1 = range(1, len(history_1.history['val_accuracy']) + 1)\n",
        "epochs_2 = range(1, len(history_2.history['val_accuracy']) + 1)\n",
        "epochs_3 = range(1, len(history_3.history['val_accuracy']) + 1)\n",
        "\n",
        "ax4.plot(epochs_1, history_1.history['val_accuracy'],\n",
        "         label='Model 1', linewidth=2.5, color='steelblue', alpha=0.8)\n",
        "ax4.plot(epochs_2, history_2.history['val_accuracy'],\n",
        "         label='Model 2', linewidth=2.5, color='seagreen', alpha=0.8)\n",
        "ax4.plot(epochs_3, history_3.history['val_accuracy'],\n",
        "         label='Model 3', linewidth=2.5, color='purple', alpha=0.8)\n",
        "\n",
        "ax4.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
        "ax4.set_ylabel('Validation Accuracy', fontsize=12, fontweight='bold')\n",
        "ax4.set_title('Learning Curves Comparison', fontsize=14, fontweight='bold')\n",
        "ax4.legend(loc='lower right', fontsize=11)\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================\n",
        "# PART C: Per-Character Accuracy Heatmap\n",
        "# ============================================\n",
        "print(f\"\\n PART C: Per-Character Performance Heatmap...\\n\")\n",
        "\n",
        "# Create matrix of accuracies\n",
        "accuracy_matrix = np.array([\n",
        "    class_accuracy_1,\n",
        "    class_accuracy_2,\n",
        "    class_accuracy_3\n",
        "])\n",
        "\n",
        "plt.figure(figsize=(18, 6))\n",
        "sns.heatmap(accuracy_matrix,\n",
        "            xticklabels=CLASS_NAMES,\n",
        "            yticklabels=['Model 1', 'Model 2', 'Model 3'],\n",
        "            annot=True, fmt='.1f', cmap='RdYlGn',\n",
        "            vmin=80, vmax=100,\n",
        "            cbar_kws={'label': 'Accuracy (%)'})\n",
        "plt.title('Per-Character Accuracy Heatmap (All Models)',\n",
        "         fontsize=16, fontweight='bold', pad=20)\n",
        "plt.xlabel('Character', fontsize=12)\n",
        "plt.ylabel('Model', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================\n",
        "# PART D: Error Analysis\n",
        "# ============================================\n",
        "print(f\"\\n PART D: Error Analysis...\\n\")\n",
        "\n",
        "# Calculate error rates\n",
        "error_rate_1 = (1 - test_accuracy_1) * 100\n",
        "error_rate_2 = (1 - test_accuracy_2) * 100\n",
        "error_rate_3 = (1 - test_accuracy_3) * 100\n",
        "\n",
        "print(\"Error Rate Comparison:\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"  Model 1: {error_rate_1:.2f}% error rate ({int(error_rate_1 * len(X_test) / 100)} errors)\")\n",
        "print(f\"  Model 2: {error_rate_2:.2f}% error rate ({int(error_rate_2 * len(X_test) / 100)} errors)\")\n",
        "print(f\"  Model 3: {error_rate_3:.2f}% error rate ({int(error_rate_3 * len(X_test) / 100)} errors)\")\n",
        "\n",
        "# Error reduction\n",
        "error_reduction_2_vs_1 = ((error_rate_1 - error_rate_2) / error_rate_1) * 100\n",
        "error_reduction_3_vs_1 = ((error_rate_1 - error_rate_3) / error_rate_1) * 100\n",
        "\n",
        "print(f\"\\nError Reduction:\")\n",
        "print(f\"  Model 2 vs Model 1: {error_reduction_2_vs_1:.1f}% reduction ‚úì\")\n",
        "print(f\"  Model 3 vs Model 1: {error_reduction_3_vs_1:.1f}% reduction ‚úì\")\n",
        "\n",
        "# Find characters that all models struggle with\n",
        "all_models_accuracy = pd.DataFrame({\n",
        "    'Character': CLASS_NAMES,\n",
        "    'Model_1': class_accuracy_1,\n",
        "    'Model_2': class_accuracy_2,\n",
        "    'Model_3': class_accuracy_3,\n",
        "    'Average': [(a+b+c)/3 for a,b,c in zip(class_accuracy_1, class_accuracy_2, class_accuracy_3)]\n",
        "}).sort_values('Average')\n",
        "\n",
        "print(\"\\n  Most Challenging Characters (All Models):\")\n",
        "print(\"-\" * 80)\n",
        "print(all_models_accuracy.head(10).to_string(index=False))\n",
        "\n",
        "print(\"\\n Easiest Characters (All Models):\")\n",
        "print(\"-\" * 80)\n",
        "print(all_models_accuracy.tail(10).to_string(index=False))\n",
        "\n",
        "# ============================================\n",
        "# PART E: Improvement Analysis\n",
        "# ============================================\n",
        "print(f\"\\n PART E: Progressive Improvement Analysis...\\n\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 6))\n",
        "\n",
        "x = np.arange(NUM_CLASSES)\n",
        "width = 0.25\n",
        "\n",
        "bars1 = ax.bar(x - width, class_accuracy_1, width, label='Model 1',\n",
        "               alpha=0.7, color='steelblue')\n",
        "bars2 = ax.bar(x, class_accuracy_2, width, label='Model 2',\n",
        "               alpha=0.7, color='seagreen')\n",
        "bars3 = ax.bar(x + width, class_accuracy_3, width, label='Model 3',\n",
        "               alpha=0.7, color='purple')\n",
        "\n",
        "ax.axhline(y=np.mean(class_accuracy_1), color='steelblue',\n",
        "          linestyle='--', linewidth=2, alpha=0.5, label=f'Model 1 Avg: {np.mean(class_accuracy_1):.1f}%')\n",
        "ax.axhline(y=np.mean(class_accuracy_2), color='seagreen',\n",
        "          linestyle='--', linewidth=2, alpha=0.5, label=f'Model 2 Avg: {np.mean(class_accuracy_2):.1f}%')\n",
        "ax.axhline(y=np.mean(class_accuracy_3), color='purple',\n",
        "          linestyle='--', linewidth=2, alpha=0.5, label=f'Model 3 Avg: {np.mean(class_accuracy_3):.1f}%')\n",
        "\n",
        "ax.set_xlabel('Character', fontsize=12)\n",
        "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
        "ax.set_title('Per-Character Performance: Progressive Improvement',\n",
        "            fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(CLASS_NAMES, rotation=90, fontsize=9)\n",
        "ax.legend(loc='lower right', fontsize=10)\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================\n",
        "# PART F: Recommendations & Insights\n",
        "# ============================================\n",
        "print(f\"\\n PART F: Insights & Recommendations\\n\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"KEY FINDINGS:\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"1. Best Performing Model: {model_names[best_model_idx]}\")\n",
        "print(f\"   ‚Üí Achieved {accuracies[best_model_idx]*100:.2f}% accuracy\")\n",
        "print(f\"   ‚Üí {error_rate_3:.2f}% error rate on test set\")\n",
        "print()\n",
        "\n",
        "# Accuracy improvement\n",
        "improvement_1_to_2 = (test_accuracy_2 - test_accuracy_1) * 100\n",
        "improvement_2_to_3 = (test_accuracy_3 - test_accuracy_2) * 100\n",
        "improvement_1_to_3 = (test_accuracy_3 - test_accuracy_1) * 100\n",
        "\n",
        "print(f\"2. Progressive Improvements:\")\n",
        "print(f\"   ‚Üí Model 1‚Üí2: +{improvement_1_to_2:.2f}% accuracy gain\")\n",
        "print(f\"   ‚Üí Model 2‚Üí3: +{improvement_2_to_3:.2f}% accuracy gain\")\n",
        "print(f\"   ‚Üí Model 1‚Üí3: +{improvement_1_to_3:.2f}% total improvement\")\n",
        "print()\n",
        "\n",
        "# Efficiency analysis\n",
        "best_efficiency = None\n",
        "if accuracies[1] > accuracies[0] and (training_time_2 - training_time_1) < 60:\n",
        "    best_efficiency = \"Model 2\"\n",
        "    print(f\"3. Efficiency Winner: Model 2\")\n",
        "    print(f\"   ‚Üí Only +{(training_time_2-training_time_1)/60:.2f} min slower than Model 1\")\n",
        "    print(f\"   ‚Üí But gained +{improvement_1_to_2:.2f}% accuracy\")\n",
        "    print(f\"   ‚Üí Best accuracy-to-time ratio\")\n",
        "else:\n",
        "    best_efficiency = model_names[best_model_idx]\n",
        "    print(f\"3. Best Overall: {model_names[best_model_idx]}\")\n",
        "    print(f\"   ‚Üí Highest accuracy justifies training time\")\n",
        "\n",
        "print()\n",
        "print(f\"4. Character Recognition:\")\n",
        "chars_perfect = sum([1 for acc in class_accuracy_3 if acc == 100])\n",
        "chars_above_95 = sum([1 for acc in class_accuracy_3 if acc >= 95])\n",
        "chars_below_90 = sum([1 for acc in class_accuracy_3 if acc < 90])\n",
        "\n",
        "print(f\"   ‚Üí {chars_perfect}/{NUM_CLASSES} characters: 100% accuracy\")\n",
        "print(f\"   ‚Üí {chars_above_95}/{NUM_CLASSES} characters: ‚â•95% accuracy\")\n",
        "print(f\"   ‚Üí {chars_below_90}/{NUM_CLASSES} characters: <90% accuracy\")\n",
        "print()\n",
        "\n",
        "print(\"RECOMMENDATIONS:\")\n",
        "print(\"-\" * 70)\n",
        "print(f\" For Production: Use {model_names[best_model_idx]}\")\n",
        "print(f\"   ‚Üí Highest accuracy: {accuracies[best_model_idx]*100:.2f}%\")\n",
        "print(f\"   ‚Üí Most reliable predictions\")\n",
        "print()\n",
        "print(f\" For Fast Deployment: Use {best_efficiency}\")\n",
        "print(f\"   ‚Üí Good balance of speed and accuracy\")\n",
        "print()\n",
        "print(\" For Further Improvement:\")\n",
        "print(\"   ‚Üí Collect more samples for low-performing characters\")\n",
        "print(\"   ‚Üí Apply additional augmentation techniques\")\n",
        "print(\"   ‚Üí Consider ensemble methods (combine models)\")\n",
        "print(\"   ‚Üí Try transfer learning with pre-trained models\")\n",
        "\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ============================================\n",
        "# PART G: Save Final Results\n",
        "# ============================================\n",
        "print(f\"\\n PART G: Saving Final Results...\\n\")\n",
        "\n",
        "# Create final summary dictionary\n",
        "final_results = {\n",
        "    'models': {\n",
        "        'model_1': model_1_results,\n",
        "        'model_2': model_2_results,\n",
        "        'model_3': model_3_results\n",
        "    },\n",
        "    'best_model': model_names[best_model_idx],\n",
        "    'best_accuracy': float(accuracies[best_model_idx]),\n",
        "    'comparison': comparison_df.to_dict(),\n",
        "    'insights': {\n",
        "        'chars_perfect': int(chars_perfect),\n",
        "        'chars_above_95': int(chars_above_95),\n",
        "        'chars_below_90': int(chars_below_90),\n",
        "        'improvement_1_to_3': float(improvement_1_to_3)\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"‚úì Final results compiled and ready\")\n",
        "print(f\"‚úì Best model: {model_names[best_model_idx]}\")\n",
        "print(f\"‚úì Best accuracy: {accuracies[best_model_idx]*100:.2f}%\")\n",
        "\n",
        "# ============================================\n",
        "# Final Summary\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\" ANALYSIS COMPLETE!\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"  Total Models Trained:  3\")\n",
        "print(f\"  Best Model:            {model_names[best_model_idx]}\")\n",
        "print(f\"  Best Accuracy:         {accuracies[best_model_idx]*100:.2f}%\")\n",
        "print(f\"  Total Training Time:   {(training_time_1+training_time_2+training_time_3)/60:.2f} minutes\")\n",
        "print(f\"  Dataset Size:          {len(X_train):,} training samples\")\n",
        "print(f\"  Augmentation:          5x (original + 4 augmented)\")\n",
        "print(f\"  Classes:               {NUM_CLASSES} Chinese characters\")\n"
      ],
      "metadata": {
        "id": "iL3Y-5R4hLKq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}